{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain_chroma\n",
    "# ! pip install sentence_transformers\n",
    "# ! pip install faiss-cpu\n",
    "! pip install -q groq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RAG system is a system that help our llm to generalize in a specifique content or local content(private).\n",
    "building a rag system require the steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. devide the document into chunks to suit the llm context window\n",
    "2. embedd the chunks and then store them in a vector db\n",
    "3. after the query arrive, embedd it then retreive k-chunks that has similarity with the query\n",
    "4. finaly include the k-chunks into the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the data, pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"./loi-n-01-00-portant-organisation-de-lenseignement-supérieur.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"président est prépondérante. \\n \\nArticle 12 \\nLe conseil de l'université délibère sur toutes les questions relatives aux missions et à la bonne marche de l'université. \\nA cet effet, et outre les attributions qui lui sont dévolues par la présente loi, il :\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[5].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the second part is to devide the text to chunks, we will use the langchaine text splitter for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=80)\n",
    "splits = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(splits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the embedding models from langchain require api_key, which may cost me something, i will do an alternative which is using a free embedding model from hugginface as they are open source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because the suitable format for huggin face model are plaint text, i transformed the splited text which is of type -langchain_core.documents.base.Document-, to simple text chuncks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = [doc.page_content for doc in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"20 \\nuniversités prévu à l'article 17 ci-dessus, les personnels de l' Etat sus-mentionnés demeurent ré gis par les statuts particulier s \\ndont ils relèvent. \\n \\nArticle 91 \\nLa situation conférée par le statut des personnels des universités aux personnels transférés en vertu de l'article 90 ci-dessus ne saurait, en \\naucun cas, être moins favorable que leur situation statutaire à la date de leur transfert. \\n \\nArticle 92 \\nLes services effectués par les personnels visés à l'article 90 dans les universités, dans les établissements universitaires et dans \\nl'administration sont considérés comme ayant été effectués au sein des universités. \\n \\nArticle 93 \\nNonobstant toutes dispositions contraires, les personnels transférés ou intégrés aux universités en application des dispositions\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Embedding generation and store them to the vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for this task i will be using Aleph Alpha's semantic embeddings from the langchain embedding models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\Desktop\\MS_DS\\M1\\S2\\AppAutomaique\\Projet\\loi001_project\\myenv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [model.encode(chunk) for chunk in text_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_array = np.array(embeddings) #to store it into faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109, 384)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the embedding into a vector DB using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = embedding_array.shape[1]  # Dimension of your embeddings\n",
    "index = faiss.IndexFlatL2(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(embedding_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieving the chuncks with most similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieving_chunks(query):\n",
    "    # Example query\n",
    "    # query = \"Quelles sont les prestations et le financement des services sociaux destinés aux étudiants dans le cadre de la vie universitaire ?\"\n",
    "    query_embedding = model.encode(query)  # Generate embedding for the query\n",
    "\n",
    "    # Convert to NumPy array and reshape for FAISS\n",
    "    query_embedding = np.array([query_embedding], dtype=np.float32)\n",
    "\n",
    "    # Perform the search\n",
    "    k = 3  # Number of nearest neighbors to retrieve\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    matching_chunks = [text_chunks[i] for i in indices[0]]\n",
    "    return '/n'.join(matching_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "    \n",
    "api_key = config['api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_generation(question, context):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            # Set an optional system message. This sets the behavior of the\n",
    "            # assistant and can be used to provide specific instructions for\n",
    "            # how it should behave throughout the conversation.\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"repondre au question en se basant sur le context donnée\"\n",
    "            },\n",
    "            # Set a user message for the assistant to respond to.\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"question : \"+question+\",context: \"+context\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-70b-8192\",\n",
    "    )\n",
    "\n",
    "    print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selon le contexte donné, les prestations des services sociaux destinés aux étudiants dans le cadre de la vie universitaire comprennent :\n",
      "\n",
      "* L'hébergement\n",
      "* La restauration\n",
      "* La couverture sanitaire\n",
      "* Les bourses et prêts d'études\n",
      "* Un système de bourses destiné aux étudiants méritants démunis\n",
      "\n",
      "Le financement de ces services sociaux est assuré par :\n",
      "\n",
      "* Des subventions de l'Etat\n",
      "* Des collectivités locales\n",
      "* Des établissements d'enseignement supérieur\n",
      "* La participation des bénéficiaires\n",
      "* Des contributions volontaires de personnes physiques ou morales.\n"
     ]
    }
   ],
   "source": [
    "respoons = llm_generation('Quelles sont les prestations et le financement des services sociaux destinés aux étudiants dans le cadre de la vie universitaire ?',ret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
