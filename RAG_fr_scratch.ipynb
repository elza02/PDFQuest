{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain_chroma\n",
    "# ! pip install sentence_transformers\n",
    "# ! pip install faiss-cpu\n",
    "! pip install -q groq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RAG system is a system that help our llm to generalize in a specifique content or local content(private).\n",
    "building a rag system require the steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. devide the document into chunks to suit the llm context window\n",
    "2. embedd the chunks and then store them in a vector db\n",
    "3. after the query arrive, embedd it then retreive k-chunks that has similarity with the query\n",
    "4. finaly include the k-chunks into the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\Desktop\\MS_DS\\M1\\S2\\AppAutomaique\\Projet\\loi001_project\\myenv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from groq import Groq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the data, pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"./loi-n-01-00-portant-organisation-de-lenseignement-supérieur.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"9 \\nIl veille, sous la supervision du président de l'université, au re spect de la législation et de la réglementation en vigueur et  du \\nrèglement intérieur dans l'enceinte de l'établissement et pe ut prendre toutes les mesures que les circonstances exigent \\nconformément à la législation en vigueur. \\n \\nArticle 22 \\nLe conseil de l'établissement comprend des membres de droit, des représentants élus des personnels enseignants et des \\npersonnels administratifs et techniques,  des représentants élus des étudiants, ainsi que des membres désignés parmi des \\npersonnalités extérieures. \\nLa composition des conseils des établissements, le mode de désignation ou d'élection de leurs membres, ainsi que les \\nmodalités de leur fonctionnement sont fixés par voie réglementaire. \\nOutre les attributions qui lui sont dévolues par la présente loi, le conseil de l'établissement: \\n9 connaît de toutes les questions relatives aux missions et à la bonne marche de l'établissement et peut formuler des \\npropositions au conseil de l'université ; \\n9 élabore les propositions budgétaires de l'établissement; \\n9 assure la répartition des moyens budgétaires entre les différentes structures visées au 2e alinéa de l'article 19 ci-dessus; \\n9 adopte les projets de création de laboratoires ; \\n9 élabore le régime des études et des examens et des contrôles de connaissances des formations assurées et les soumet à \\nl'approbation selon les modalités prévues à l'article 8 ci-dessus; \\n9 exerce le pouvoir disciplinaire à l'égard des étudiants dans les conditions déterminées par voie réglementaire; \\n9 propose au conseil de l'université les mesures propres à améliorer l'insertion professionnelle des diplômés; \\n9 propose au conseil de l'université toute réforme des formations assurées au sein de l'établissement et prend toute mesure \\nde nature pédagogique visant la qualité de la formation ; \\n9 propose au conseil de l'université les mesures visant à amé liorer l'orientation et l'information des étudiants et à \\nencourager l'organisation des activités culturelles et sportives; \\n9 délibère sur toutes les questions relatives aux missions et à la bonne marche de l'établissement; \\n9 prend toutes mesures visant à améliorer la gestion de l'établissement ; \\n9 soumet à l'approbation du conseil de l'université les propositions de création des centres ; \\n9 élabore son règlement intérieur qui est soumis au conseil de l'université pour approbation ; \\n9 crée en son sein des commissions permanentes dont une co mmission de la recherche, une commission pédagogique, une \\ncommission de suivi budgétaire et une comm ission scientifique et, le cas échéant, des commissions ad hoc. Le nombre, \\nla composition et les modalités de fonctionnement des commis sions permanentes sont fixés dans le règlement intérieur \\nde l'établissement, sous réserve des dispositions de l'article 23 ci-dessous. \\n \\nArticle 23 \\nLa commission scientifique de chaque établissement universitaire est chargée de proposer toutes les mesures concernant le \\npersonnel enseignant-chercheur notamment en ce qui concerne leur titularisation, leur avancement et leur discipline. \\nLa composition de cette commission, son fonctionnement et les modalités de désignation et d'élection de ses membres sont \\nfixés par voie réglementaire, sous réserve de la parité entre les membres désignés et les membres élus. \\n \\nArticle 24 \\nLes structures d'enseignement et de recherche, les structures administratives de chaque établissement universitaire, leur \\norganisation et les conditions de nomination aux différentes st ructures administratives sont fixées par le conseil de \\nl'université sur proposition du conseil de l'établissement. \\n \\nChapitre III \\nDes établissements d'enseignement supérieur \\nne relevant pas des universités \\n \\nArticle 25 \\nLes établissements d'enseignement supérieur ne relevant pas des universités et qui relèvent ou sont sous tutelle de différents \\ndépartements ministériels ont pour missions principales :\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[10].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the second part is to devide the text to chunks, we will use the langchaine text splitter for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=80)\n",
    "splits = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(splits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the embedding models from langchain require api_key, which may cost me something, i will do an alternative which is using a free embedding model from hugginface as they are open source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because the suitable format for huggin face model are plaint text, i transformed the splited text which is of type -langchain_core.documents.base.Document-, to simple text chuncks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = [doc.page_content for doc in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Embedding generation and store them to the vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for this task i will be using Aleph Alpha's semantic embeddings from the langchain embedding models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [model.encode(chunk) for chunk in text_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_array = np.array(embeddings) #to store it into faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109, 384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the embedding into a vector DB using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = embedding_array.shape[1]  # Dimension of your embeddings\n",
    "index = faiss.IndexFlatL2(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(embedding_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieving the chuncks with most similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieving_chunks(query):\n",
    "    # Example query\n",
    "    # query = \"Quelles sont les prestations et le financement des services sociaux destinés aux étudiants dans le cadre de la vie universitaire ?\"\n",
    "    query_embedding = model.encode(query)  # Generate embedding for the query\n",
    "\n",
    "    # Convert to NumPy array and reshape for FAISS\n",
    "    query_embedding = np.array([query_embedding], dtype=np.float32)\n",
    "\n",
    "    # Perform the search\n",
    "    k = 3  # Number of nearest neighbors to retrieve\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    matching_chunks = [text_chunks[i] for i in indices[0]]\n",
    "    return '/n'.join(matching_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "    \n",
    "api_key = config['api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_generation(question, context):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            # Set an optional system message. This sets the behavior of the\n",
    "            # assistant and can be used to provide specific instructions for\n",
    "            # how it should behave throughout the conversation.\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"repondre au question en se basant sur le context donnée\"\n",
    "            },\n",
    "            # Set a user message for the assistant to respond to.\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"question : \"+question+\",context: \"+context\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-70b-8192\",\n",
    "    )\n",
    "\n",
    "    print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = retrieving_chunks(query= 'donner moi article 75')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 75 : Dans les cinq ans suivant le prononcé d'une décision de condamnation, une personne qui a été condamnée pour une infraction prévue aux articles 63 et 64 et au présent article, et qui commet une infraction de même nature, sera considérée comme récidiviste.\n"
     ]
    }
   ],
   "source": [
    "respoons = llm_generation('donner moi article 75',ret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
